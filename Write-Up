## Writeup

---

**Advanced Lane Finding Project**

The goals / steps of this project are the following:

* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.
* Apply a distortion correction to raw images.
* Use color transforms, gradients, etc., to create a thresholded binary image.
* Apply a perspective transform to rectify binary image ("birds-eye view").
* Detect lane pixels and fit to find the lane boundary.
* Determine the curvature of the lane and vehicle position with respect to center.
* Warp the detected lane boundaries back onto the original image.
* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.

## [Rubric](https://review.udacity.com/#!/rubrics/571/view) Points

### Here I will consider the rubric points individually and describe how I addressed each point in my implementation.  

Example images are provided in the zip folder provided in the submission. 

### Writeup / README

#### 1. Provide a Writeup / README that includes all the rubric points and how you addressed each one.  You can submit your writeup as markdown or pdf.  [Here](https://github.com/udacity/CarND-Advanced-Lane-Lines/blob/master/writeup_template.md) is a template writeup for this project you can use as a guide and a starting point.  

You're reading it!

### Camera Calibration

#### 1. Briefly state how you computed the camera matrix and distortion coefficients. Provide an example of a distortion corrected calibration image.

The code for this step is contained in the second cell of the IPython notebook titled "P2.ipynb"

I start by preparing "object points", which will be the (x, y, z) coordinates of the chessboard corners in the world. Here I am assuming the chessboard is fixed on the (x, y) plane at z=0, such that the object points are the same for each calibration image.  Thus, `objp` is just a replicated array of coordinates, and `objpoints` will be appended with a copy of it every time I successfully detect all chessboard corners in a test image.  `imgpoints` will be appended with the (x, y) pixel position of each of the corners in the image plane with each successful chessboard detection.  

I then used the output `objpoints` and `imgpoints` to compute the camera calibration and distortion coefficients using the `cv2.calibrateCamera()` function.  I applied this distortion correction to the test image using the `cv2.undistort()` function.

"calibration2.jpg"


### Pipeline (single images)

The pipeline for generating the images is located in cell 3 of the notebook "P2.ipynb". I iterated through each test image using a for loop. 

#### 1. Provide an example of a distortion-corrected image.

Lines 5-13. 
To undistort the image, I applied the 'cv2.undisort()' function using the 'mtx' and 'dist' variables from the camera calibration step. 

"undistorted_straight_lines1.jpg"


#### 2. Describe how (and identify where in your code) you used color transforms, gradients or other methods to create a thresholded binary image.  Provide an example of a binary image result.

Lines 16-62. 
I used a combination of color and gradient thresholds to generate a binary image. First, I converted the undistorted image from the RGB to HLS colorspace to take advantage of the S channel. I set thresholds of 170 and 255 in the s channel and masked the remaining pixels. In parallel, I converted the undistorted image to grayscale and applied a Sobel threshold of 15-255 in the x-direction using the 'cv2.Sobel()' function. The remaining pixels were masked; this eliminated the lines at or near horizontal. I then combined the two filters to produce a binary file. 

"binary_straight_lines1.jpg"


#### 3. Describe how (and identify where in your code) you performed a perspective transform and provide an example of a transformed image.

Lines 65-132. 
To transform the perspective and obtain birds-eye views of the lanes, I warped the perspective using the function 'cv2.warpPerspective'. I chose source points that when tranformed would produce relatively parallel lines. To do this, I used a test image with straight lanes and produced an output with vertical lane lines. 


This resulted in the following source and destination points:

| Source        | Destination   | 
|:-------------:|:-------------:| 
| 595, 450      | 310, 0        | 
| 180, 720      | 310, 720      |
| 1125, 720     | 960, 720      |
| 688, 450      | 960, 0        |

Based on the source and destination, I used function 'cv2.getPerspectiveTransform' to produce a transformation matrix, M. This matrix was applied to each image. With M known, the image was then transformed. I performed these steps first with the color image then with the binary image. 

"warped_straight_lines1.jpg"
"warped_binary_straight_lines1.jpg"


#### 4. Describe how (and identify where in your code) you identified lane-line pixels and fit their positions with a polynomial?

Lines 134-298. 
I began this section by creating a polygon mask, similarly to how I did in project 1 "Finding Lane Lines." I did this by choosing points around the lane lines and masking all other points. This mask was applied to the undistorted image, then the image was warped to produce a masked, warped image. (Lines 136-162)

I then built a histogram that shows the number of pixels per x coordinate in the bottom half of the warped binary image. The purpose of this was to use it to determine where the pixels are located. Then, I used a sliding window method to determine a polynominal curve to fit to each lane line. This method uses 9 windows each with a margin of 80 in the x-direction to search for pixels. The window position updates with the average pixel location. Second order polynomial lines were then fit to the lanes using 'np.polyfit()'. (Lines 164-238)

Next, to streamline the method, I used a method to search from the prior polynomial position. I defined a margin of 80 around the polynomial to search for pixels, as opposed to searching within the windows from before. I then produced new polynomials again using 'np.polyfit()'. (Lines 240-298) 

"lane_lines_straight_lines1.jpg"


#### 5. Describe how (and identify where in your code) you calculated the radius of curvature of the lane and the position of the vehicle with respect to center.

Lines 301-328. 
First, I determined a y-direction pixel to meter relationship by observing that the lane length is about 30m while the height of the image is 720 pixels. Meanwhile, minimum lane width is 3.7m, and the images shows a lane width of about 650 pixels in the test images. Therefore the conversions are 30/720 and 3.7/650 respectively. Using these conversions, I found new polynomial lines for the lanes which are based in meters instead of pixels. To determine radius of curvature, I used the radius of curvature equation included in the Advanced Computer Vision Lesson, which utilizes the new polynomials. The radius of curvature is then the average of the radius of curvature of each polynomial line. To determine the vehicle's position relative to the middle of the lane, I determined the midpoint of the lane by averaging the two polynomial lines. The offset is then the x position of that midpoint line at the bottom of the image subtracted by half of the total x size of the image. 


#### 6. Provide an example image of your result plotted back down onto the road such that the lane area is identified clearly.

Lines 331-379. 
I created an image called 'out_img' which converts the warped_binary image back to a 3-axis color space. A file 'window_img' is a blank image with the same shape of out_img. In lines 337-353, I generated a green polygon enclosed by each of the polynomial lane lines and drew it onto the window_img. I also colored the pixels from the left and right lane lines red and blue respectively. To unwarp this green polygon and red/blue pixel points, I applied the inverse of the transformation matrix from before, Minv, and used 'cv2.warpPerspective()' again. I then used the function 'cv2.addWeighted' to overlay the unwarped green polygon and red/blue pixels onto the original test image. I overlaid the radius of curvature and lateral offset information as well using 'cv2.putText()'. 

"final_straight_lines1.jpg"


### Pipeline (video)

#### 1. Provide a link to your final video output.  Your pipeline should perform reasonably well on the entire project video (wobbly lines are ok but no catastrophic failures that would cause the car to drive off the road!).

"my_project_video.mp4"

---

### Discussion

#### 1. Briefly discuss any problems / issues you faced in your implementation of this project.  Where will your pipeline likely fail?  What could you do to make it more robust?

Issues may arise with the masked area selection. I chose the polygon points knowing where the lanes would be. However, the lane lines might not always be within the polygon; this would cause issues. One must be careful when it comes to global variables in the implementation of the pipeline. 'mtx' and 'dist' are global variables that are used in the function, but any others should be renamed accordingly so that there are variables by the same name in the function. 

To make it more robust, the masking polygon, transformation matrix, and thresholds could be altered on a frame-by-frame basis to account for differences in road color, lane color, shadows, etc.
